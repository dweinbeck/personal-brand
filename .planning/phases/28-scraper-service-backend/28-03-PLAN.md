---
phase: 28-scraper-service-backend
plan: 03
type: execute
wave: 2
depends_on: ["28-01"]
files_modified:
  - /Users/dweinbeck/Documents/brand-scraper/src/worker/handler.ts
  - /Users/dweinbeck/Documents/brand-scraper/src/pipeline/package/downloader.ts

autonomous: true

must_haves:
  truths:
    - "Each extracted asset is uploaded individually to GCS under jobs/{jobId}/assets/{category}/{filename}"
    - "An assets manifest is built during upload and persisted to the assetsManifest DB column"
    - "No automatic zip is created on job completion (individual objects only)"
    - "Asset upload emits asset_saved and asset_failed progress events"
    - "brand_json_url continues working via signed URL (no regression)"
  artifacts:
    - path: "src/worker/handler.ts"
      provides: "Refactored asset flow: download-to-buffer, upload individually to GCS, build manifest, persist to DB"
      contains: "uploadAsset"
    - path: "src/pipeline/package/downloader.ts"
      provides: "downloadAssetToBuffer function that returns buffer instead of writing to disk"
      exports: ["downloadAssetToBuffer"]
  key_links:
    - from: "src/worker/handler.ts"
      to: "src/delivery/gcs.ts"
      via: "uploadAsset() calls for each downloaded asset"
      pattern: "uploadAsset"
    - from: "src/worker/handler.ts"
      to: "src/db/schema.ts"
      via: "DB update with assetsManifest and gcsAssetsPrefix"
      pattern: "assetsManifest|gcsAssetsPrefix"
    - from: "src/worker/handler.ts"
      to: "src/pipeline/package/downloader.ts"
      via: "downloadAssetToBuffer for each asset"
      pattern: "downloadAssetToBuffer"
---

<objective>
Replace the batch temp-dir-then-zip asset flow with individual GCS uploads per asset, building an assets manifest persisted to the database.

Purpose: Requirements ASST-01, ASST-02, ASST-03 require individual asset storage in GCS (no automatic zip). This enables the frontend (Phase 29/30) to display individual asset previews and the zip endpoint (Plan 04) to create zips on demand from the manifest.

Output: New downloadAssetToBuffer function in downloader.ts. Refactored handler asset section that uploads individually, emits events, and stores manifest.
</objective>

<execution_context>
@/Users/dweinbeck/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dweinbeck/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@/Users/dweinbeck/Documents/personal-brand/.planning/ROADMAP.md
@/Users/dweinbeck/Documents/personal-brand/.planning/phases/28-scraper-service-backend/28-RESEARCH.md
@/Users/dweinbeck/Documents/personal-brand/.planning/phases/28-scraper-service-backend/28-01-SUMMARY.md

IMPORTANT: This phase targets the brand-scraper repo at /Users/dweinbeck/Documents/brand-scraper/. All file paths are relative to that repo.

NOTE: This plan runs in parallel with Plan 02. Plan 02 modifies the event wiring and pipeline execution section of handler.ts. This plan modifies the GCS upload section of handler.ts (the section after pipeline completes, inside the `if (gcsBucket)` block). If Plan 02 has already run, the handler will have onEvent wiring -- use ctx.onEvent for asset events. If Plan 02 has NOT run yet, still add ctx.onEvent?.() calls (they will no-op until Plan 02 wires them). Read handler.ts fresh before modifying.

Key source files to read:
@/Users/dweinbeck/Documents/brand-scraper/src/worker/handler.ts
@/Users/dweinbeck/Documents/brand-scraper/src/pipeline/package/downloader.ts
@/Users/dweinbeck/Documents/brand-scraper/src/delivery/gcs.ts
@/Users/dweinbeck/Documents/brand-scraper/src/db/schema.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add downloadAssetToBuffer function to downloader</name>
  <files>
    /Users/dweinbeck/Documents/brand-scraper/src/pipeline/package/downloader.ts
  </files>
  <action>
In /Users/dweinbeck/Documents/brand-scraper/src/pipeline/package/downloader.ts:

1. Add a new exported type for buffer download results:
```typescript
export interface AssetBufferResult {
  originalUrl: string;
  /** Relative path like 'logos/primary.svg' used for GCS naming */
  localPath: string;
  success: boolean;
  error?: string;
  contentType: string;
  sizeBytes: number;
  buffer: Buffer;
}
```

2. Add a new exported function `downloadAssetToBuffer` that downloads a single asset and returns a buffer instead of writing to disk. This reuses the existing validation logic but returns the buffer:

```typescript
/**
 * Download a single asset and return it as a buffer with metadata.
 * Reuses the same validation pipeline as downloadSingleAsset (content-type check,
 * size limit, magic byte validation, extension correction) but returns a Buffer
 * instead of writing to disk.
 *
 * Used by the worker handler for individual GCS uploads.
 */
export async function downloadAssetToBuffer(
  asset: AssetToDownload,
  ctx: PipelineContext,
): Promise<AssetBufferResult> {
  const localPath = determineLocalPath(asset);

  // Handle data: URLs
  if (asset.url.startsWith("data:")) {
    return downloadDataUrlToBuffer(asset.url, localPath, ctx);
  }

  try {
    const response = await fetch(asset.url, {
      signal: AbortSignal.timeout(DOWNLOAD_TIMEOUT_MS),
      headers: { "User-Agent": ctx.crawlConfig.userAgent },
    });

    if (!response.ok) {
      const errorMsg = `HTTP ${response.status} ${response.statusText}`;
      ctx.logger.warn({ url: asset.url, status: response.status }, "asset download failed");
      return { originalUrl: asset.url, localPath, success: false, error: errorMsg, contentType: "", sizeBytes: 0, buffer: Buffer.alloc(0) };
    }

    const contentType = response.headers.get("content-type")?.split(";")[0].trim() ?? "";
    if (!isAllowedContentType(contentType)) {
      const errorMsg = `Unexpected content type: ${contentType}`;
      ctx.logger.warn({ url: asset.url, contentType }, "skipping asset with unexpected content type");
      return { originalUrl: asset.url, localPath, success: false, error: errorMsg, contentType, sizeBytes: 0, buffer: Buffer.alloc(0) };
    }

    const buffer = Buffer.from(await response.arrayBuffer());

    if (buffer.length > MAX_ASSET_SIZE) {
      const errorMsg = `Asset too large: ${buffer.length} bytes (max ${MAX_ASSET_SIZE})`;
      ctx.logger.warn({ url: asset.url, sizeBytes: buffer.length }, "asset too large, skipping");
      return { originalUrl: asset.url, localPath, success: false, error: errorMsg, contentType, sizeBytes: buffer.length, buffer: Buffer.alloc(0) };
    }

    const validation = validateImageContent(buffer);
    if (!validation.valid) {
      const errorMsg = "Invalid image content (failed magic byte check)";
      ctx.logger.warn({ url: asset.url, contentType }, "skipping asset with invalid image content");
      return { originalUrl: asset.url, localPath, success: false, error: errorMsg, contentType, sizeBytes: buffer.length, buffer: Buffer.alloc(0) };
    }

    const finalPath = correctExtension(localPath, contentType, validation.detectedFormat);

    ctx.logger.debug(
      { url: asset.url, localPath: finalPath, sizeBytes: buffer.length },
      "asset downloaded to buffer",
    );

    return {
      originalUrl: asset.url,
      localPath: finalPath,
      success: true,
      contentType,
      sizeBytes: buffer.length,
      buffer,
    };
  } catch (err) {
    const errorMsg = err instanceof Error ? err.message : String(err);
    ctx.logger.warn({ url: asset.url, error: errorMsg }, "asset download error");
    return { originalUrl: asset.url, localPath, success: false, error: errorMsg, contentType: "", sizeBytes: 0, buffer: Buffer.alloc(0) };
  }
}
```

3. Add a private helper for data URL buffer handling (similar to existing downloadDataUrl but returns buffer):
```typescript
function downloadDataUrlToBuffer(
  dataUrl: string,
  localPath: string,
  ctx: PipelineContext,
): AssetBufferResult {
  try {
    const match = dataUrl.match(/^data:([^;,]+)?(?:;base64)?,(.*)$/);
    if (!match) {
      return {
        originalUrl: dataUrl.slice(0, 50),
        localPath,
        success: false,
        error: "Invalid data URL format",
        contentType: "",
        sizeBytes: 0,
        buffer: Buffer.alloc(0),
      };
    }

    const contentType = match[1] ?? "application/octet-stream";
    const data = match[2];
    const isBase64 = dataUrl.includes(";base64,");

    const buffer = isBase64
      ? Buffer.from(data, "base64")
      : Buffer.from(decodeURIComponent(data), "utf-8");

    if (buffer.length > MAX_ASSET_SIZE) {
      return {
        originalUrl: dataUrl.slice(0, 50),
        localPath,
        success: false,
        error: `Data URL too large: ${buffer.length} bytes`,
        contentType,
        sizeBytes: buffer.length,
        buffer: Buffer.alloc(0),
      };
    }

    const finalPath = contentType.includes("svg")
      ? localPath.replace(/\.[^.]+$/, ".svg")
      : localPath;

    ctx.logger.debug({ localPath: finalPath, sizeBytes: buffer.length }, "data URL asset to buffer");

    return {
      originalUrl: dataUrl.slice(0, 50),
      localPath: finalPath,
      success: true,
      contentType,
      sizeBytes: buffer.length,
      buffer,
    };
  } catch (err) {
    const errorMsg = err instanceof Error ? err.message : String(err);
    return {
      originalUrl: dataUrl.slice(0, 50),
      localPath,
      success: false,
      error: errorMsg,
      contentType: "",
      sizeBytes: 0,
      buffer: Buffer.alloc(0),
    };
  }
}
```

IMPORTANT: Do NOT modify the existing `downloadAssets` or `downloadSingleAsset` functions. They are used by the CLI and must remain working.
  </action>
  <verify>
1. `cd /Users/dweinbeck/Documents/brand-scraper && npx tsc --noEmit` passes
2. downloadAssetToBuffer and AssetBufferResult are exported: `grep -n "export.*downloadAssetToBuffer\|export.*AssetBufferResult" /Users/dweinbeck/Documents/brand-scraper/src/pipeline/package/downloader.ts`
3. Existing downloadAssets function unchanged: `grep -n "export async function downloadAssets" /Users/dweinbeck/Documents/brand-scraper/src/pipeline/package/downloader.ts`
  </verify>
  <done>
- downloadAssetToBuffer function downloads single asset to buffer with full validation pipeline
- downloadDataUrlToBuffer handles inline data URLs returning buffers
- AssetBufferResult type exported with buffer, contentType, sizeBytes fields
- Existing disk-writing download functions untouched
  </done>
</task>

<task type="auto">
  <name>Task 2: Refactor handler GCS section to upload assets individually and build manifest</name>
  <files>
    /Users/dweinbeck/Documents/brand-scraper/src/worker/handler.ts
  </files>
  <action>
Read handler.ts FRESH before modifying (Plan 02 may have already modified it).

In /Users/dweinbeck/Documents/brand-scraper/src/worker/handler.ts:

1. Add imports for the new functions and types:
```typescript
import { uploadAsset } from "../delivery/gcs.js";
import { downloadAssetToBuffer, type AssetBufferResult } from "../pipeline/package/downloader.js";
import type { AssetManifestEntry, AssetsManifest } from "../db/schema.js";
```

Note: Keep the existing `downloadAssets` import if it's still there (for backward compatibility), but it won't be used in the new flow. You can remove the `downloadAssets` import and the `mkdtemp`, `rm`, `tmpdir`, `join`, `PassThrough`, `archiver` imports since they are no longer needed. Also remove the `createInMemoryZip` function since it's replaced by on-demand zip.

2. Replace the entire GCS upload section inside `if (gcsBucket)`. Currently this section:
   - Downloads assets to temp dir
   - Creates in-memory zip
   - Calls uploadResults() for brand.json + assets.zip
   - Updates DB with GCS URIs

Replace it with:

```typescript
if (gcsBucket) {
  try {
    // Upload brand.json (keep existing flow via uploadResults, no assets zip)
    const brandJsonBuffer = Buffer.from(JSON.stringify(result.taxonomy, null, 2));
    const expiryMs = Number(process.env.GCS_SIGNED_URL_EXPIRY_MS) || 3_600_000;
    const gcsResult = await uploadResults(
      gcsBucket,
      jobId,
      brandJsonBuffer,
      null, // No automatic zip -- ASST-03
      expiryMs,
    );

    brandJsonSignedUrl = gcsResult.brandJsonSignedUrl;

    // Update DB with brand.json URI (no assets zip URI on initial upload)
    await db
      .update(jobs)
      .set({
        gcsBrandJsonUri: gcsResult.brandJsonUri,
      })
      .where(eq(jobs.id, jobId));

    jobLogger.info({ brandJsonUri: gcsResult.brandJsonUri }, "brand.json uploaded to GCS");

    // Upload individual assets to GCS and build manifest
    const downloadableAssets = extractDownloadableAssets(result.taxonomy);
    const manifestEntries: AssetManifestEntry[] = [];
    const gcsAssetsPrefix = `jobs/${jobId}/assets/`;

    for (const asset of downloadableAssets) {
      const downloadResult = await downloadAssetToBuffer(asset, ctx);

      if (downloadResult.success) {
        try {
          // Parse category and filename from localPath (e.g., "logos/primary.svg")
          const pathParts = downloadResult.localPath.split("/");
          const category = pathParts[0]; // "logos", "favicons", "og"
          const filename = pathParts.slice(1).join("/"); // "primary.svg"

          const uploadResult = await uploadAsset(
            gcsBucket,
            jobId,
            category,
            filename,
            downloadResult.buffer,
            downloadResult.contentType,
          );

          manifestEntries.push({
            category,
            filename,
            originalUrl: downloadResult.originalUrl,
            contentType: downloadResult.contentType,
            sizeBytes: downloadResult.sizeBytes,
            gcsObjectPath: uploadResult.objectPath,
          });

          // Emit asset_saved event
          await ctx.onEvent?.({
            type: "asset_saved",
            timestamp: new Date().toISOString(),
            detail: { filename: `${category}/${filename}`, sizeBytes: downloadResult.sizeBytes },
          });

          jobLogger.debug({ objectPath: uploadResult.objectPath }, "asset uploaded to GCS");
        } catch (uploadErr) {
          // Upload failure for a single asset is non-fatal
          jobLogger.warn(
            { url: downloadResult.originalUrl, error: String(uploadErr) },
            "asset GCS upload failed",
          );
          await ctx.onEvent?.({
            type: "asset_failed",
            timestamp: new Date().toISOString(),
            detail: { url: downloadResult.originalUrl, error: String(uploadErr) },
          });
        }
      } else {
        // Download failed
        await ctx.onEvent?.({
          type: "asset_failed",
          timestamp: new Date().toISOString(),
          detail: { url: asset.url, error: downloadResult.error },
        });
      }
    }

    // Build and persist assets manifest
    const assetsManifest: AssetsManifest = {
      assets: manifestEntries,
      totalCount: manifestEntries.length,
      totalSizeBytes: manifestEntries.reduce((sum, e) => sum + e.sizeBytes, 0),
      createdAt: new Date().toISOString(),
    };

    await db
      .update(jobs)
      .set({
        gcsAssetsPrefix,
        assetsManifest,
      })
      .where(eq(jobs.id, jobId));

    jobLogger.info(
      { assetCount: manifestEntries.length, totalBytes: assetsManifest.totalSizeBytes },
      "assets uploaded and manifest persisted",
    );
  } catch (err) {
    jobLogger.error(
      { error: String(err) },
      "GCS upload failed, continuing without GCS artifacts",
    );
    // GCS failure is non-fatal: job is already marked as succeeded/partial in DB
  }
}
```

3. Remove the `createInMemoryZip` function (no longer needed -- zip is on-demand now).

4. Clean up imports: Remove unused imports that were only needed for the old zip flow:
   - `mkdtemp`, `rm` from "node:fs/promises" (if no longer used)
   - `tmpdir` from "node:os" (if no longer used)
   - `join` from "node:path" (if no longer used)
   - `PassThrough` from "node:stream" (if no longer used)
   - `archiver` from "archiver" (if no longer used)
   - `downloadAssets` from downloader (replaced by downloadAssetToBuffer)

Keep `uploadResults` import -- it's still used for brand.json upload.

IMPORTANT:
- Keep the `assetsZipSignedUrl` variable in the webhook payload section. Set it to null since we no longer create automatic zips. The webhook payload still references it.
- The `assetsZipSignedUrl` variable should remain `null` (initialized as `let assetsZipSignedUrl: string | null = null` -- no change needed since we no longer set it).
- Do NOT remove or modify the webhook delivery section.
  </action>
  <verify>
1. `cd /Users/dweinbeck/Documents/brand-scraper && npx tsc --noEmit` passes
2. Old zip flow removed: `grep -c "createInMemoryZip\|mkdtemp\|tmpdir" /Users/dweinbeck/Documents/brand-scraper/src/worker/handler.ts` returns 0
3. New flow present: `grep -c "uploadAsset\|downloadAssetToBuffer\|assetsManifest\|gcsAssetsPrefix" /Users/dweinbeck/Documents/brand-scraper/src/worker/handler.ts` returns multiple matches
4. brand.json upload preserved: `grep -c "uploadResults" /Users/dweinbeck/Documents/brand-scraper/src/worker/handler.ts` returns at least 1
5. `cd /Users/dweinbeck/Documents/brand-scraper && npm test` passes
  </verify>
  <done>
- Handler downloads each asset to buffer via downloadAssetToBuffer
- Each asset uploaded individually to GCS via uploadAsset under jobs/{jobId}/assets/{category}/{filename}
- No automatic zip created on completion (ASST-03)
- Assets manifest built incrementally and persisted to assetsManifest column
- gcsAssetsPrefix stored in DB
- asset_saved/asset_failed events emitted per asset
- brand.json upload path preserved via uploadResults (SAPI-03 regression protection)
- Old temp-dir + in-memory zip code removed
  </done>
</task>

</tasks>

<verification>
1. TypeScript compiles: `cd /Users/dweinbeck/Documents/brand-scraper && npx tsc --noEmit`
2. Tests pass: `cd /Users/dweinbeck/Documents/brand-scraper && npm test`
3. No temp dir usage in handler: `grep -c "mkdtemp\|tmpdir" /Users/dweinbeck/Documents/brand-scraper/src/worker/handler.ts` returns 0
4. Individual uploads present: `grep "uploadAsset" /Users/dweinbeck/Documents/brand-scraper/src/worker/handler.ts`
5. Manifest persistence present: `grep "assetsManifest" /Users/dweinbeck/Documents/brand-scraper/src/worker/handler.ts`
6. brand.json still uploaded: `grep "uploadResults" /Users/dweinbeck/Documents/brand-scraper/src/worker/handler.ts`
</verification>

<success_criteria>
- Each asset downloaded to buffer and uploaded individually to GCS
- Assets stored at jobs/{jobId}/assets/{category}/{filename} path pattern
- Assets manifest built and persisted to DB with totalCount, totalSizeBytes, createdAt
- No automatic zip creation on job completion
- asset_saved/asset_failed events emitted per asset
- brand.json upload unchanged (no regression)
- Old temp-dir and in-memory zip code removed
- TypeScript compiles, tests pass
</success_criteria>

<output>
After completion, create `/Users/dweinbeck/Documents/personal-brand/.planning/phases/28-scraper-service-backend/28-03-SUMMARY.md`
</output>
