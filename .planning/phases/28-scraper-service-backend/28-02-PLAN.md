---
phase: 28-scraper-service-backend
plan: 02
type: execute
wave: 2
depends_on: ["28-01"]
files_modified:
  - /Users/dweinbeck/Documents/brand-scraper/src/worker/handler.ts
  - /Users/dweinbeck/Documents/brand-scraper/src/pipeline/orchestrator.ts

autonomous: true

must_haves:
  truths:
    - "During a scrape job, progress events are emitted at each pipeline stage boundary"
    - "Events are persisted to pipelineMeta.events JSONB during processing, not only at the end"
    - "Events include page_started, page_done for each crawled page"
    - "The events array is capped at 200 entries to prevent JSONB bloat"
  artifacts:
    - path: "src/worker/handler.ts"
      provides: "Event emitter callback wired into pipeline context, events array management, incremental DB persistence"
      contains: "onEvent"
    - path: "src/pipeline/orchestrator.ts"
      provides: "Progress event emission at page_started, page_done, extract_done, assembly_done boundaries"
      contains: "onEvent"
  key_links:
    - from: "src/worker/handler.ts"
      to: "src/pipeline/context.ts"
      via: "ctx.onEvent callback assignment"
      pattern: "ctx\\.onEvent"
    - from: "src/pipeline/orchestrator.ts"
      to: "src/pipeline/context.ts"
      via: "ctx.onEvent?.() calls at stage boundaries"
      pattern: "ctx\\.onEvent\\?\\."
    - from: "src/worker/handler.ts"
      to: "src/db/schema.ts"
      via: "DB update writing events to pipelineMeta.events"
      pattern: "pipelineMeta.*events"
---

<objective>
Wire progress event emission throughout the pipeline and persist events to the database incrementally during job processing.

Purpose: Requirements PROG-01 and PROG-02 require that the scraper service emits and persists progress events during pipeline execution (not only at the end). This enables the frontend (Phase 29) to show live progress to users.

Output: Handler creates onEvent callback that appends to in-memory array and flushes to DB. Orchestrator emits events at page_started, page_done, extract_done, assembly_done boundaries.
</objective>

<execution_context>
@/Users/dweinbeck/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dweinbeck/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@/Users/dweinbeck/Documents/personal-brand/.planning/ROADMAP.md
@/Users/dweinbeck/Documents/personal-brand/.planning/phases/28-scraper-service-backend/28-RESEARCH.md
@/Users/dweinbeck/Documents/personal-brand/.planning/phases/28-scraper-service-backend/28-01-SUMMARY.md

IMPORTANT: This phase targets the brand-scraper repo at /Users/dweinbeck/Documents/brand-scraper/. All file paths are relative to that repo.

Key source files to read:
@/Users/dweinbeck/Documents/brand-scraper/src/worker/handler.ts
@/Users/dweinbeck/Documents/brand-scraper/src/pipeline/orchestrator.ts
@/Users/dweinbeck/Documents/brand-scraper/src/pipeline/context.ts
@/Users/dweinbeck/Documents/brand-scraper/src/db/schema.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Wire onEvent callback in handler and emit pipeline_started/pipeline_done</name>
  <files>
    /Users/dweinbeck/Documents/brand-scraper/src/worker/handler.ts
  </files>
  <action>
In /Users/dweinbeck/Documents/brand-scraper/src/worker/handler.ts:

1. Import ProgressEvent type from schema:
```typescript
import type { ProgressEvent } from "../db/schema.js";
```

2. Create a helper function (inside handler.ts, not exported) that manages the events array and persists to DB:
```typescript
/**
 * Create an onEvent callback that accumulates events in-memory
 * and flushes to pipelineMeta.events in the DB.
 * Caps at 200 events to prevent JSONB bloat.
 */
function createEventEmitter(
  db: Database,
  jobId: string,
  logger: FastifyBaseLogger,
): { onEvent: (event: ProgressEvent) => Promise<void>; events: ProgressEvent[] } {
  const events: ProgressEvent[] = [];

  const onEvent = async (event: ProgressEvent): Promise<void> => {
    events.push(event);
    // Cap at 200 entries, trimming oldest
    if (events.length > 200) {
      events.splice(0, events.length - 200);
    }

    try {
      await db
        .update(jobs)
        .set({
          pipelineMeta: sql`jsonb_set(
            COALESCE(pipeline_meta, '{}'::jsonb),
            '{events}',
            ${JSON.stringify(events)}::jsonb
          )`,
        })
        .where(eq(jobs.id, jobId));
    } catch (err) {
      // Event persistence failure is non-fatal -- log and continue
      logger.warn({ error: String(err), eventType: event.type }, "failed to persist progress event");
    }
  };

  return { onEvent, events };
}
```

You will need to add an import for `sql` from drizzle-orm:
```typescript
import { eq, sql } from "drizzle-orm";
```

3. In the `handleJob` function, AFTER creating the pipeline context (`const ctx = createContext(siteUrl)`), wire up the event emitter:
```typescript
// Wire progress event emitter
const { onEvent, events } = createEventEmitter(db, jobId, jobLogger);
ctx.onEvent = onEvent;

// Emit pipeline_started
await onEvent({
  type: "pipeline_started",
  timestamp: new Date().toISOString(),
  detail: { siteUrl },
});
```

4. In the success path (after `const result = await runPipeline(siteUrl, ctx)`), emit pipeline_done BEFORE the existing DB update:
```typescript
await onEvent({
  type: "pipeline_done",
  timestamp: new Date().toISOString(),
  detail: { success: result.success, stageCount: result.stages.length },
});
```

5. In the existing success DB update (the one that sets status, completedAt, result, pipelineMeta), merge the events array into the pipelineMeta object:
```typescript
pipelineMeta: {
  stages: result.stages.map((s) => ({
    stage: s.stage,
    status: s.status,
    duration_ms: s.duration_ms,
  })),
  pages_sampled: result.stages.filter((s) => s.stage === "crawl").length,
  duration_ms,
  events, // <-- ADD THIS: include accumulated events
},
```

This ensures the final DB write includes both stage metadata AND the full events array.

IMPORTANT: Do NOT change the GCS upload section or webhook section of the handler. Those will be modified in Plan 03. Only touch the pipeline execution and initial DB update sections.
  </action>
  <verify>
1. `cd /Users/dweinbeck/Documents/brand-scraper && npx tsc --noEmit` passes
2. Grep confirms: `grep -n "onEvent" /Users/dweinbeck/Documents/brand-scraper/src/worker/handler.ts` shows callback creation and wiring
3. Grep confirms: `grep -n "pipeline_started\|pipeline_done" /Users/dweinbeck/Documents/brand-scraper/src/worker/handler.ts` shows both events emitted
4. Grep confirms: `grep -n "events" /Users/dweinbeck/Documents/brand-scraper/src/worker/handler.ts` shows events included in pipelineMeta
  </verify>
  <done>
- createEventEmitter helper creates callback that accumulates events + persists to DB
- Events array capped at 200 entries
- pipeline_started emitted before runPipeline
- pipeline_done emitted after runPipeline
- Final pipelineMeta DB write includes events array
  </done>
</task>

<task type="auto">
  <name>Task 2: Emit progress events from pipeline orchestrator at stage boundaries</name>
  <files>
    /Users/dweinbeck/Documents/brand-scraper/src/pipeline/orchestrator.ts
  </files>
  <action>
In /Users/dweinbeck/Documents/brand-scraper/src/pipeline/orchestrator.ts:

Add progress event emissions at these specific locations. Use the optional chaining pattern `await ctx.onEvent?.({ ... })` so the pipeline continues to work without events when onEvent is not set (e.g., CLI usage).

1. **page_started before homepage crawl** -- Right before `homePage = await crawlPage(browserContext, url, ctx)` (inside the try block):
```typescript
await ctx.onEvent?.({
  type: "page_started",
  timestamp: new Date().toISOString(),
  detail: { url, isHomepage: true },
});
```

2. **page_done after homepage crawl** -- Right after the successful homepage crawl stage push (after `ctx.logger.info({ duration_ms: crawlDuration }, "homepage crawl complete")`):
```typescript
await ctx.onEvent?.({
  type: "page_done",
  timestamp: new Date().toISOString(),
  detail: { url, isHomepage: true, duration_ms: crawlDuration },
});
```

3. **page_started before each sampled page crawl** -- Inside the `for (const pageUrl of sampled)` loop, right after `await rateLimiter.wait()` and BEFORE `const page = await crawlPage(...)`:
```typescript
await ctx.onEvent?.({
  type: "page_started",
  timestamp: new Date().toISOString(),
  detail: { url: pageUrl, isHomepage: false },
});
```

4. **page_done after each sampled page** -- After `await page.close()` and the existing logger.info in the successful branch of the loop:
```typescript
await ctx.onEvent?.({
  type: "page_done",
  timestamp: new Date().toISOString(),
  detail: { url: pageUrl, isHomepage: false },
});
```

5. **page_done on sampled page failure** -- In the catch block of the sampled page loop, after the logger.warn:
```typescript
await ctx.onEvent?.({
  type: "page_done",
  timestamp: new Date().toISOString(),
  detail: { url: pageUrl, isHomepage: false, failed: true, error: String(err) },
});
```

6. **extract_done after all extraction + merge** -- After `const merged = mergeExtractionResults(allResults)` and the extract stage push:
```typescript
await ctx.onEvent?.({
  type: "extract_done",
  timestamp: new Date().toISOString(),
  detail: { extractorCount: merged.size, pagesProcessed: allResults.length },
});
```

7. **assembly_done after successful assembly** -- After the assemble stage push in the success path (after `ctx.logger.info(... "assemble stage complete")`):
```typescript
await ctx.onEvent?.({
  type: "assembly_done",
  timestamp: new Date().toISOString(),
  detail: { valid: validation.valid },
});
```

IMPORTANT:
- Use optional chaining (`ctx.onEvent?.()`) everywhere so the pipeline works without events wired.
- Do NOT restructure the orchestrator logic. Only ADD event emissions at the listed points.
- Each event emission is a single `await` statement -- do not wrap in try/catch (the handler's onEvent callback already handles errors).
  </action>
  <verify>
1. `cd /Users/dweinbeck/Documents/brand-scraper && npx tsc --noEmit` passes
2. Count event emissions: `grep -c "ctx.onEvent" /Users/dweinbeck/Documents/brand-scraper/src/pipeline/orchestrator.ts` should return 7
3. Verify all event types present: `grep "type:" /Users/dweinbeck/Documents/brand-scraper/src/pipeline/orchestrator.ts | grep -o '"[a-z_]*"'` should show page_started (x2), page_done (x3), extract_done, assembly_done
4. `cd /Users/dweinbeck/Documents/brand-scraper && npm test` passes
  </verify>
  <done>
- 7 event emissions added to orchestrator at stage boundaries
- page_started emitted before homepage crawl and before each sampled page crawl
- page_done emitted after homepage crawl, after each sampled page (success and failure)
- extract_done emitted after extraction merge
- assembly_done emitted after taxonomy assembly
- All use optional chaining for backward compatibility
  </done>
</task>

</tasks>

<verification>
1. TypeScript compiles: `cd /Users/dweinbeck/Documents/brand-scraper && npx tsc --noEmit`
2. Tests pass: `cd /Users/dweinbeck/Documents/brand-scraper && npm test`
3. Event flow is complete: pipeline_started (handler) -> page_started/page_done (orchestrator, per page) -> extract_done (orchestrator) -> assembly_done (orchestrator) -> pipeline_done (handler)
4. Events persist to DB during processing via pipelineMeta JSONB updates
</verification>

<success_criteria>
- pipeline_started and pipeline_done emitted from handler
- page_started, page_done, extract_done, assembly_done emitted from orchestrator
- Events accumulated in-memory and persisted incrementally to pipelineMeta.events
- Events array capped at 200 entries
- Event persistence failures are non-fatal (logged, not thrown)
- Pipeline still works when onEvent is not set (CLI usage)
- TypeScript compiles, tests pass
</success_criteria>

<output>
After completion, create `/Users/dweinbeck/Documents/personal-brand/.planning/phases/28-scraper-service-backend/28-02-SUMMARY.md`
</output>
