---
phase: 13-proxy-integration
plan: 02
type: execute
wave: 2
depends_on: ["13-01"]
files_modified:
  - src/app/api/assistant/chat/route.ts
autonomous: false

must_haves:
  truths:
    - "User sends a message in the assistant chat and receives an answer from the FastAPI RAG backend"
    - "The chat loading indicator shows while waiting for the FastAPI response and disappears when the answer renders"
    - "If the FastAPI service is unavailable or returns an error, the user sees a clear error message (not a frozen chat)"
    - "The assistant page works identically in local development and production (env var driven)"
  artifacts:
    - path: "src/app/api/assistant/chat/route.ts"
      provides: "Proxy route handler that forwards questions to FastAPI and returns UIMessageStream responses"
      exports: ["POST"]
  key_links:
    - from: "src/app/api/assistant/chat/route.ts"
      to: "src/lib/assistant/fastapi-client.ts"
      via: "import askFastApi, FastApiError"
      pattern: "import.*askFastApi.*from.*fastapi-client"
    - from: "src/app/api/assistant/chat/route.ts"
      to: "createUIMessageStream"
      via: "import from ai package"
      pattern: "import.*createUIMessageStream.*from.*ai"
    - from: "ChatInterface.tsx (useChat)"
      to: "src/app/api/assistant/chat/route.ts"
      via: "DefaultChatTransport POST to /api/assistant/chat"
      pattern: "api/assistant/chat"
---

<objective>
Rewrite the API route handler to proxy requests to the FastAPI RAG backend instead of calling Gemini directly. The frontend (`ChatInterface.tsx`, `useChat`, `DefaultChatTransport`) remains completely unchanged.

Purpose: This is the core integration that makes the assistant answer questions using the external RAG backend. After this plan, end-to-end chat works through the proxy.
Output: Rewritten `src/app/api/assistant/chat/route.ts` + visual verification
</objective>

<execution_context>
@/Users/dweinbeck/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dweinbeck/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/13-proxy-integration/13-RESEARCH.md
@.planning/phases/13-proxy-integration/13-01-SUMMARY.md

# Files being modified
@src/app/api/assistant/chat/route.ts

# Frontend (unchanged, but understand the contract)
@src/components/assistant/ChatInterface.tsx

# Existing schema used for request validation
@src/lib/schemas/assistant.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Rewrite route handler as FastAPI proxy</name>
  <files>src/app/api/assistant/chat/route.ts</files>
  <action>
Completely rewrite `src/app/api/assistant/chat/route.ts` to proxy to FastAPI instead of calling Gemini. The file keeps the same path and HTTP interface (POST) so the frontend is unchanged.

Replace the ENTIRE file contents with this implementation:

```typescript
export const dynamic = "force-dynamic";

import { createUIMessageStream, createUIMessageStreamResponse } from "ai";
import { askFastApi, FastApiError } from "@/lib/assistant/fastapi-client";
import { chatRequestSchema } from "@/lib/schemas/assistant";

export async function POST(request: Request) {
  // 1. Parse and validate request body (same schema as before)
  let body: unknown;
  try {
    body = await request.json();
  } catch {
    return new Response(
      JSON.stringify({ error: "Invalid JSON in request body." }),
      { status: 400, headers: { "Content-Type": "application/json" } },
    );
  }

  const parsed = chatRequestSchema.safeParse(body);
  if (!parsed.success) {
    return new Response(
      JSON.stringify({
        error: "Invalid request.",
        details: parsed.error.flatten().fieldErrors,
      }),
      { status: 400, headers: { "Content-Type": "application/json" } },
    );
  }

  // 2. Extract the last user message text from UIMessage parts
  const lastUserMsg = parsed.data.messages.findLast((m) => m.role === "user");
  const question =
    lastUserMsg?.parts
      ?.filter((p) => p.type === "text" && p.text)
      .map((p) => p.text)
      .join("") ?? "";

  if (!question) {
    return new Response(
      JSON.stringify({ error: "No question provided." }),
      { status: 400, headers: { "Content-Type": "application/json" } },
    );
  }

  // 3. Call FastAPI via the typed client
  try {
    const data = await askFastApi(question);

    // 4. Build response text with citations appended as markdown
    let text = data.answer;
    if (data.citations.length > 0) {
      text += "\n\n---\n**Sources:**\n";
      for (const cite of data.citations) {
        text += `- ${cite.source}\n`;
      }
    }

    // 5. Return as UIMessageStream (same protocol useChat expects)
    const stream = createUIMessageStream({
      execute: ({ writer }) => {
        writer.write({
          type: "text-delta",
          delta: text,
          id: "fastapi-response",
        });
      },
    });
    return createUIMessageStreamResponse({ stream });
  } catch (err) {
    // 6. Map FastApiError to user-friendly responses
    if (err instanceof FastApiError) {
      let userMessage: string;
      if (err.isTimeout) {
        userMessage =
          "The assistant is taking too long to respond. Please try again.";
      } else if (err.status === 429) {
        userMessage = "Too many messages. Please wait a moment.";
      } else if (err.status === 503) {
        userMessage =
          "The assistant is currently unavailable. Please try again later.";
      } else {
        userMessage = "Something went wrong. Please try again.";
      }

      return new Response(JSON.stringify({ error: userMessage }), {
        status: err.status >= 500 ? 502 : err.status,
        headers: { "Content-Type": "application/json" },
      });
    }

    // Unexpected error
    return new Response(
      JSON.stringify({ error: "An unexpected error occurred." }),
      { status: 500, headers: { "Content-Type": "application/json" } },
    );
  }
}
```

Critical details:
- REMOVE all old imports: `streamText`, `headers`, `assistantModel`, `MODEL_CONFIG`, `buildSystemPrompt`, `checkAssistantRateLimit`, `logConversation`, `runSafetyPipeline`. These become dead code handled in Phase 15.
- KEEP `chatRequestSchema` import -- the request validation stays the same.
- KEEP `export const dynamic = "force-dynamic"` -- still needed for the POST handler.
- Extract question from `parts` array (NOT `content` string) -- this is the AI SDK v5 format.
- Use `askFastApi()` from Plan 01's client wrapper -- handles fetch, timeout, validation.
- Append citations as markdown for now (Phase 14 adds structured citation UI).
- Map `FastApiError` to user-friendly messages with appropriate HTTP status codes.
- The field is `delta` (NOT `textDelta`) in the `text-delta` chunk type.
- `getClientIp()` function is removed -- rate limiting is now handled by FastAPI.
  </action>
  <verify>
1. `npx tsc --noEmit` -- no type errors
2. `npm run lint` -- passes
3. `npm run build` -- builds successfully
4. Verify no old imports remain: `grep -c "gemini\|safety\|prompts\|rate-limit\|logging" src/app/api/assistant/chat/route.ts` should return 0
5. Verify new imports exist: `grep "askFastApi" src/app/api/assistant/chat/route.ts` should match
  </verify>
  <done>
`src/app/api/assistant/chat/route.ts` is rewritten as a thin proxy. Old Gemini/safety/logging imports removed. New `askFastApi` client imported. Request validation unchanged. Response returned as UIMessageStream. Error handling maps FastApiError to user-friendly messages. Build passes.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: Verify end-to-end chat works</name>
  <what-built>
The assistant chat API route has been rewritten to proxy requests to the FastAPI RAG backend. The frontend is completely unchanged -- `ChatInterface.tsx`, `useChat`, and `DefaultChatTransport` all remain as-is.
  </what-built>
  <how-to-verify>
1. Ensure `CHATBOT_API_URL` is set in `.env.local` to the FastAPI Cloud Run service URL (e.g., `https://chatbot-assistant-XXXXX-uc.a.run.app`)
2. Run `npm run dev` to start the local dev server
3. Navigate to http://localhost:3000/assistant
4. Type a question like "What projects has Dan worked on?" and send it
5. Verify:
   - The loading indicator appears while waiting for the response
   - An answer appears from the FastAPI RAG backend (content will be about Dan's GitHub repos, not generic)
   - If citations are returned, they appear as a "Sources:" section at the bottom of the response
   - The loading indicator disappears when the answer renders
6. Test error handling:
   - Stop the FastAPI service (or set `CHATBOT_API_URL` to an invalid URL)
   - Send a message -- verify you see a clear error message, not a frozen chat
7. Verify the page works when `CHATBOT_API_URL` is unset:
   - Remove the env var from `.env.local`
   - Send a message -- verify you see "The assistant is currently unavailable" error
  </how-to-verify>
  <resume-signal>Type "approved" if chat works end-to-end, or describe any issues.</resume-signal>
</task>

</tasks>

<verification>
1. `npm run build` passes with no errors
2. `npm run lint` passes with no errors
3. No old Gemini/safety imports remain in route.ts
4. Route handler imports `askFastApi` from the client wrapper created in Plan 01
5. Route handler uses `createUIMessageStream` + `createUIMessageStreamResponse` (same pattern as before)
6. Human verification confirms end-to-end chat works
</verification>

<success_criteria>
- User sends a message and receives a FastAPI-powered answer
- Loading indicator shows during request and disappears on response
- Error states produce clear user-facing messages
- Build, lint, and type check all pass
- Zero frontend changes required
</success_criteria>

<output>
After completion, create `.planning/phases/13-proxy-integration/13-02-SUMMARY.md`
</output>
