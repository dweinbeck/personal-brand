---
phase: 05-seo-polish-and-stubs
plan: 02
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - src/app/sitemap.ts
  - src/app/robots.ts
autonomous: true

must_haves:
  truths:
    - "Crawlers can discover all site pages via /sitemap.xml"
    - "Crawlers know the site allows full indexing via /robots.txt"
    - "Lighthouse scores 90+ on Performance, Accessibility, Best Practices, and SEO"
  artifacts:
    - path: "src/app/sitemap.ts"
      provides: "Dynamic sitemap with all static routes and tutorial slugs"
      exports: ["default"]
      contains: "MetadataRoute.Sitemap"
    - path: "src/app/robots.ts"
      provides: "Robots config allowing all crawlers"
      exports: ["default"]
      contains: "MetadataRoute.Robots"
  key_links:
    - from: "src/app/sitemap.ts"
      to: "src/lib/tutorials.ts"
      via: "getAllTutorials import"
      pattern: "getAllTutorials"
    - from: "src/app/robots.ts"
      to: "/sitemap.xml"
      via: "sitemap URL reference"
      pattern: "dweinbeck\\.com/sitemap\\.xml"
---

<objective>
Create sitemap.xml and robots.txt convention files, then run a Lighthouse audit to verify the site meets the 90+ score threshold across all categories.

Purpose: Ensures search engines can discover and index all pages, and confirms the site meets performance/accessibility/SEO quality bar.
Output: Two convention files (sitemap.ts, robots.ts) and a verified Lighthouse audit confirming 90+ scores.
</objective>

<execution_context>
@/Users/dweinbeck/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dweinbeck/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-seo-polish-and-stubs/05-RESEARCH.md
@.planning/phases/05-seo-polish-and-stubs/05-01-SUMMARY.md
@src/lib/tutorials.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create sitemap.ts and robots.ts convention files</name>
  <files>
    src/app/sitemap.ts
    src/app/robots.ts
  </files>
  <action>
    **src/app/sitemap.ts:**
    Create a convention file that exports an async default function returning `Promise<MetadataRoute.Sitemap>`.

    - Import `MetadataRoute` from "next" and `getAllTutorials` from "@/lib/tutorials"
    - Define `const BASE_URL = "https://dweinbeck.com"`
    - Call `getAllTutorials()` to get dynamic tutorial slugs
    - Map tutorials to sitemap entries: `url: \`\${BASE_URL}/building-blocks/\${t.slug}\``, lastModified from `t.metadata.publishedAt`, changeFrequency "monthly", priority 0.7
    - Return array of static routes plus tutorial entries:
      - `/` -- priority 1, changeFrequency "monthly"
      - `/projects` -- priority 0.8, changeFrequency "weekly"
      - `/building-blocks` -- priority 0.8, changeFrequency "weekly"
      - `/contact` -- priority 0.5, changeFrequency "yearly"
      - `/writing` -- priority 0.5, changeFrequency "monthly"
      - `/assistant` -- priority 0.3, changeFrequency "monthly"
      - ...tutorialUrls spread at end
    - Use `new Date()` for lastModified on static routes

    **src/app/robots.ts:**
    Create a convention file that exports a default function returning `MetadataRoute.Robots`.

    - Import `MetadataRoute` from "next"
    - Return: rules with userAgent "*" and allow "/", sitemap "https://dweinbeck.com/sitemap.xml"

    Keep imports alphabetical (Biome rule).
  </action>
  <verify>
    `npx biome check src/app/sitemap.ts src/app/robots.ts` passes.
    `npm run build` succeeds.
    After build, start the production server (`npm run start`) and:
    - `curl http://localhost:3000/sitemap.xml` returns valid XML with all routes including tutorial slugs
    - `curl http://localhost:3000/robots.txt` returns "User-agent: *\nAllow: /" with sitemap reference
  </verify>
  <done>
    /sitemap.xml returns valid XML listing all static routes and dynamic tutorial routes.
    /robots.txt returns valid robots config allowing all crawlers with sitemap reference.
  </done>
</task>

<task type="auto">
  <name>Task 2: Lighthouse audit and fix any issues below 90</name>
  <files>
    (files depend on audit results -- may modify any file with accessibility or performance issues)
  </files>
  <action>
    1. Build and start production server: `npm run build && npm run start`
    2. Run Lighthouse CLI against http://localhost:3000 in headless mode:
       `npx lighthouse http://localhost:3000 --output=json --output-path=./.lighthouse-report.json --chrome-flags="--headless --no-sandbox" --only-categories=performance,accessibility,best-practices,seo`
    3. Parse the JSON report and check scores for Performance, Accessibility, Best Practices, SEO.
    4. If ALL scores are >= 90, the task is done. Record the scores in the summary.
    5. If any score is below 90, examine the audit details to identify specific failures and fix them. Common fixes:
       - Accessibility: add missing alt text, fix color contrast, add aria-labels, add lang attribute (already present), ensure form labels
       - SEO: add meta description (already done in Plan 01), ensure viewport meta (Next.js adds automatically), check crawlable links
       - Performance: check image sizes, ensure next/font is working, verify no render-blocking resources
       - Best Practices: check for console errors, ensure HTTPS-ready links, check image aspect ratios
    6. After fixes, rebuild and re-run Lighthouse to confirm all scores >= 90.
    7. Clean up: remove `.lighthouse-report.json` after recording scores.

    NOTE: If Lighthouse CLI is not available or has issues in the environment, document the scores from a manual `npm run build` output and note that Lighthouse should be run manually. The metadata and convention files from Task 1 and Plan 01 are the primary deliverables -- Lighthouse is verification.

    NOTE: Do NOT install Lighthouse globally. Use `npx lighthouse` which downloads it temporarily.
  </action>
  <verify>
    Lighthouse scores are >= 90 for all four categories: Performance, Accessibility, Best Practices, SEO.
    Or: if Lighthouse cannot run in the environment, `npm run build` succeeds and all metadata/convention files are verified via curl.
  </verify>
  <done>
    Lighthouse audit completed with all scores >= 90, or issues identified and fixed until scores meet threshold.
    Any fixes are committed alongside the convention files.
  </done>
</task>

</tasks>

<verification>
1. `npm run build` completes without errors
2. `curl localhost:3000/sitemap.xml` returns XML with all routes
3. `curl localhost:3000/robots.txt` returns valid robots.txt
4. Lighthouse scores >= 90 across Performance, Accessibility, Best Practices, SEO
</verification>

<success_criteria>
- sitemap.xml is accessible and lists all static routes plus dynamic tutorial slugs
- robots.txt is accessible with correct allow rules and sitemap reference
- Lighthouse scores meet the 90+ threshold across all four categories
- No Biome or TypeScript errors in the codebase
</success_criteria>

<output>
After completion, create `.planning/phases/05-seo-polish-and-stubs/05-02-SUMMARY.md`
</output>
